---
title: "Double/Debiased Machine Learning"
author: "Tom Ben-Shahar"
format: pdf
editor: visual
---

# Basics

Consider the following partially linear model:

$$
Y = \alpha D + g(X) + \epsilon
$$

$$
D = m(X) + v
$$

where

$Y$ :

:   Outcome of interest

$D$ :

:   Treatment or policy variable of interest

$X$ :

:   High-dimensional vector of control variables

$\alpha$ :

:   Parameter targeted for estimation (treatment effect)

$\epsilon, v$ :

:   Error terms

$m, g$ :

:   Unknown "nuisance" functions

## Naive Approach

To estimate the causal parameter $\alpha$, the given sample of data can be split into two smaller samples, which we can call the **main sample** and the **auxiliary sample**. We then use the auxiliary sample to train a simple ML algorithm to estimate the nuisance function $g(X)$ . Given the estimated function $\hat{g}(X)$, we can use the main sample to estimate the parameter $\alpha$ such that

$$
\hat{\alpha} = \left( \frac{1}{n} \sum_{i \in I_{main}}{D^2_i} \right)^{-1} \frac{1}{n} \sum_{i \in I_{main}}D_i (Y_i - \hat{g}(X_i))
$$

However, this naive approach introduces **regularization bias**.

::: callout-note
## Regularization Bias

Bias Arises..
:::
